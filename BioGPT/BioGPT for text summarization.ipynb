{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2ada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BioGptForCausalLM, AutoModelForCausalLM\n",
    "\n",
    "#checkpoint = \"gpt2-medium\"\n",
    "#checkpoint = \"gpt2-large\"\n",
    "checkpoint = \"microsoft/biogpt\"\n",
    "#checkpoint = \"microsoft/BioGPT-Large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "#model = BioGptForCausalLM.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b75e70",
   "metadata": {},
   "source": [
    "## Check Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "findings_example = f\"\"\"Lateral view somewhat limited due to overlying motion artifact. The lungs are low in volume.  There is no focal airspace consolidation to suggest pneumonia.  A 1.2-cm calcified granuloma just below the medial aspect of the right hemidiaphragm is unchanged from prior study.  No pleural effusions or pulmonary edema. There is no pneumothorax.  The inferior sternotomy wire is fractured but unchanged. Surgical clips and vascular markers in the thorax are related to prior CABG surgery.\"\"\"\n",
    "eval_prompt = findings_example + prompt\n",
    "print(f\"Model Input:\\n{eval_prompt}\\n\")\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"---\\nGenerated Output:\\n\")\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_length=256)[0], skip_special_tokens=True).split(\":\")[-1].strip())\n",
    "    \n",
    "ground_truth_summary=\"\"\"\n",
    "---\n",
    "Ground Truth:\n",
    "No evidence of acute cardiopulmonary process.\n",
    "\"\"\"    \n",
    "print(ground_truth_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35a911",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a45ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datasets\n",
    "#dataset_config = 'mimic-cxr','mimic-iii'  \n",
    "#split = 'train','validate',test\n",
    "\n",
    "max_input_length = 768\n",
    "\n",
    "def build_dataset(dataset_config, tokenizer, split):\n",
    "    data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "    findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "    impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "    image_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'image.tok')\n",
    "\n",
    "\n",
    "    findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "    impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "\n",
    "    dataset = datasets.Dataset.from_dict({\"text\":findings,\"summary\":impression}) \n",
    "    \n",
    "    \n",
    "    def preprocess_function(samples):\n",
    "        texts = samples[\"text\"]\n",
    "        summaries = samples[\"summary\"]\n",
    "        prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "        eos_token = tokenizer.eos_token\n",
    "        \n",
    "        def generate_input(_text,_summary):\n",
    "            return \" \".join([_text,prompt,_summary])\n",
    "\n",
    "        inputs = [generate_input(_text,_summary)+eos_token for _text,_summary in zip(texts,summaries)]\n",
    "        model_inputs = tokenizer(inputs)\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "    dataset = dataset.map(preprocess_function, batched=True, num_proc=4, remove_columns=list(dataset.features))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = \"mimic-cxr\"\n",
    "tokenized_train_data = build_dataset(dataset_config,tokenizer,\"train\")\n",
    "tokenized_eval_data = build_dataset(dataset_config,tokenizer,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dce51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7032eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_data = tokenized_train_data.map(group_texts, batched=True, num_proc=4)\n",
    "tokenized_eval_data = tokenized_eval_data.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad30cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "dataset_config = \"mimic-cxr\"\n",
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "batch_size = 16\n",
    "num_train_epochs = 1#5\n",
    "save_path: str=\"/nfs/turbo/umms-vgvinodv/models/finetuned-checkpoints/radsum\"\n",
    "save_path = f\"{save_path}/{model_name}-{dataset_config}\"\n",
    "\n",
    "# Training Args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    overwrite_output_dir = True,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_eval_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30d369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29547c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "findings_example = f\"\"\"Lateral view somewhat limited due to overlying motion artifact. The lungs are low in volume.  There is no focal airspace consolidation to suggest pneumonia.  A 1.2-cm calcified granuloma just below the medial aspect of the right hemidiaphragm is unchanged from prior study.  No pleural effusions or pulmonary edema. There is no pneumothorax.  The inferior sternotomy wire is fractured but unchanged. Surgical clips and vascular markers in the thorax are related to prior CABG surgery.\"\"\"\n",
    "eval_prompt = findings_example + prompt\n",
    "print(f\"Model Input:\\n{eval_prompt}\\n\")\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"---\\nGenerated Output:\\n\")\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_length=256)[0], skip_special_tokens=True).split(\":\")[-1].strip())\n",
    "    \n",
    "ground_truth_summary=\"\"\"\n",
    "---\n",
    "Ground Truth:\n",
    "No evidence of acute cardiopulmonary process.\n",
    "\"\"\"    \n",
    "print(ground_truth_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30840dce",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from radgraph import F1RadGraph\n",
    "from f1chexbert import F1CheXbert\n",
    "import datasets\n",
    "from pathlib import Path\n",
    "\n",
    "def build_test_dataset(dataset_config, tokenizer, split=\"test\"):\n",
    "    data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "    findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "    impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "\n",
    "\n",
    "    findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "    impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "\n",
    "    dataset = datasets.Dataset.from_dict({\"text\":findings,\"summary\":impression}) \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def generate_summary(sample):\n",
    "    texts = sample[\"text\"]\n",
    "    summaries = sample[\"summary\"]\n",
    "    prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "\n",
    "    def generate_input(_text):\n",
    "        return \" \".join([_text,prompt])\n",
    "\n",
    "    inputs = generate_input(texts) \n",
    "    model_input = tokenizer(inputs, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        response = tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True)\n",
    "    \n",
    "    formatted_response = response.split(\":\")[-1].strip()\n",
    "    return {\n",
    "        \"text\": inputs,\n",
    "        \"summary\":summaries,\n",
    "        \"pred\": formatted_response\n",
    "    }\n",
    "\n",
    "def process_impression(impression):\n",
    "    impression = impression.lower()\n",
    "    return ' '.join(wordpunct_tokenize(impression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_test = False\n",
    "\n",
    "test_dataset = build_test_dataset('mimic-cxr',tokenizer,'test')\n",
    "\n",
    "if mini_test:\n",
    "    num_samples = int(0.25*len(test_dataset))\n",
    "    test_dataset = test_dataset.select(range(num_samples))\n",
    "\n",
    "print(f'Number of test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24601ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "results = test_dataset.map(generate_summary, remove_columns=list(test_dataset.features))\n",
    "\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "pred_str = list(map(process_impression,pred_str))\n",
    "label_str = results[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c46d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "###################################\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "res = {key: value.mid.fmeasure * 100 for key, value in rouge_output.items()}\n",
    "print('ROUGE:')\n",
    "print({k: round(v, 4) for k, v in res.items()})\n",
    "\n",
    "#################################\n",
    "f1radgraph = F1RadGraph(reward_level=\"partial\")\n",
    "score = f1radgraph(hyps=pred_str,refs=label_str)[0]\n",
    "print(\"\\nF1RadGraph:\")\n",
    "print(score*100)\n",
    "\n",
    "#################################\n",
    "f1chexbert = F1CheXbert(device=\"cuda\")\n",
    "accuracy, accuracy_not_averaged, class_report, class_report_5 = f1chexbert(\n",
    "    hyps=pred_str,\n",
    "    refs=label_str)\n",
    "print(\"\\nF1CheXbert:\")\n",
    "print(100 * class_report_5[\"micro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159179f",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7848b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BioGptForCausalLM\n",
    "\n",
    "model_checkpoint = \"/nfs/turbo/umms-vgvinodv/models/finetuned-checkpoints/radsum/biogpt-mimic-cxr/checkpoint-7620\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = BioGptForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from radgraph import F1RadGraph\n",
    "from f1chexbert import F1CheXbert\n",
    "import datasets\n",
    "from pathlib import Path\n",
    "\n",
    "def build_test_dataset(dataset_config, tokenizer, split=\"test\"):\n",
    "    data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "    findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "    impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "\n",
    "\n",
    "    findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "    impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "\n",
    "    dataset = datasets.Dataset.from_dict({\"text\":findings,\"summary\":impression}) \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def generate_summary(sample):\n",
    "    texts = sample[\"text\"]\n",
    "    summaries = sample[\"summary\"]\n",
    "    prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "\n",
    "    def generate_input(_text):\n",
    "        return \" \".join([_text,prompt])\n",
    "\n",
    "    inputs = generate_input(texts) \n",
    "    model_input = tokenizer(inputs, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        response = tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True)\n",
    "    \n",
    "    formatted_response = response.split(\":\")[-1].strip()\n",
    "    return {\n",
    "        \"text\": inputs,\n",
    "        \"summary\":summaries,\n",
    "        \"pred\": formatted_response\n",
    "    }\n",
    "\n",
    "def process_impression(impression):\n",
    "    impression = impression.lower()\n",
    "    return ' '.join(wordpunct_tokenize(impression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f879f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_test = False\n",
    "\n",
    "test_dataset = build_test_dataset('mimic-cxr',tokenizer,'test')\n",
    "\n",
    "if mini_test:\n",
    "    num_samples = int(0.25*len(test_dataset))\n",
    "    test_dataset = test_dataset.select(range(num_samples))\n",
    "\n",
    "print(f'Number of test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_dataset.map(generate_summary, remove_columns=list(test_dataset.features))\n",
    "\n",
    "def process_impression(impression):\n",
    "    impression = impression.lower()\n",
    "    return ' '.join(wordpunct_tokenize(impression))\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "pred_str = list(map(process_impression,pred_str))\n",
    "label_str = results[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc010e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "###################################\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "res = {key: value.mid.fmeasure * 100 for key, value in rouge_output.items()}\n",
    "print('ROUGE:')\n",
    "print({k: round(v, 4) for k, v in res.items()})\n",
    "\n",
    "#################################\n",
    "f1radgraph = F1RadGraph(reward_level=\"partial\")\n",
    "score = f1radgraph(hyps=pred_str,refs=label_str)[0]\n",
    "print(\"\\nF1RadGraph:\")\n",
    "print(score*100)\n",
    "\n",
    "#################################\n",
    "f1chexbert = F1CheXbert(device=\"cuda\")\n",
    "accuracy, accuracy_not_averaged, class_report, class_report_5 = f1chexbert(\n",
    "    hyps=pred_str,\n",
    "    refs=label_str)\n",
    "print(\"\\nF1CheXbert:\")\n",
    "print(class_report_5[\"micro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8bc7c3",
   "metadata": {},
   "source": [
    "## Hidden Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd30da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "hidden_test_dataset = build_test_dataset('mimic-cxr',tokenizer,'test.hidden')\n",
    "\n",
    "print(f'Number of test samples: {len(hidden_test_dataset)}\\n')\n",
    "\n",
    "results_hidden_test = hidden_test_dataset.map(generate_summary, remove_columns=list(hidden_test_dataset.features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_str = results_hidden_test[\"pred\"]\n",
    "pred_str = list(map(process_impression,pred_str))\n",
    "label_str = results_hidden_test[\"summary\"]\n",
    "\n",
    "###################################\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "res = {key: value.mid.fmeasure * 100 for key, value in rouge_output.items()}\n",
    "print('ROUGE:')\n",
    "print({k: round(v, 4) for k, v in res.items()})\n",
    "\n",
    "#################################\n",
    "f1radgraph = F1RadGraph(reward_level=\"partial\")\n",
    "score = f1radgraph(hyps=pred_str,refs=label_str)[0]\n",
    "print(\"\\nF1RadGraph:\")\n",
    "print(score*100)\n",
    "\n",
    "#################################\n",
    "f1chexbert = F1CheXbert(device=\"cuda\")\n",
    "accuracy, accuracy_not_averaged, class_report, class_report_5 = f1chexbert(\n",
    "    hyps=pred_str,\n",
    "    refs=label_str)\n",
    "print(\"\\nF1CheXbert:\")\n",
    "print(class_report_5[\"micro avg\"][\"f1-score\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
