{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154e9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BioGptForCausalLM, AutoModelForCausalLM\n",
    "\n",
    "model_checkpoint = \"/nfs/turbo/umms-vgvinodv/models/finetuned-checkpoints/radsum/biogpt-mimic-cxr/checkpoint-7620\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "model.config.use_cache = False\n",
    "\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f69dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,437,184 || all params: 356,200,448 || trainable%: 2.649402619504847\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64, #old=8\n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1, #old=0.05\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    #target_modules = [\"q_proj\", \"v_proj\"], \n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    #target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\",\"lm_head\",], #all linear layers\n",
    ")\n",
    "\n",
    "for param in model.parameters():\n",
    "    # freeze base model's layers\n",
    "    param.requires_grad = False\n",
    "\n",
    "if hasattr(model, \"enable_input_require_grads\"):\n",
    "    model.enable_input_require_grads()\n",
    "else:\n",
    "    def make_inputs_require_grad(module, input, output):\n",
    "        output.requires_grad_(True)\n",
    "\n",
    "    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "        \n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "#print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8b5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"varuUM/mimic-cxr-dpo-with-metrics\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d5fb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_func(examples):\n",
    "    return examples['rougeL'] < 0.3\n",
    "    #return examples['rougeL'] < 0.2 and examples['F1RadGraph'] < 0.2\n",
    "\n",
    "dpo_dataset = dataset.filter(filter_func)\n",
    "dpo_dataset = dpo_dataset.remove_columns([\"rougeL\",\"F1RadGraph\",\"F1CheXbert\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c45e73c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9214\n",
      "{'prompt': 'Heart size is normal. Mediastinal contours are normal with mild aortic tortuosity. Post-surgical changes in the right hemithorax are stable including thickening of the pleura along the costal surface and blunting of the costophrenic sulcus. The right sixth rib surgical fracture is redemonstrated. There are no new lung nodules identified. The main impression based on the given FINDINGS section of the chest X-ray report are:', 'chosen': 'Stable chest radiograph.', 'rejected': 'No radiographic evidence of pneumonia.'}\n"
     ]
    }
   ],
   "source": [
    "sanity_check = True\n",
    "if sanity_check:\n",
    "    num_samples = int(0.25*len(dpo_dataset)) #10000#\n",
    "    dpo_dataset = dpo_dataset.select(range(num_samples))\n",
    "\n",
    "print(len(dpo_dataset))\n",
    "print(dpo_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b5350",
   "metadata": {},
   "source": [
    "### Remove examples with length longer thna model max input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def filter_function(examples):\n",
    "    prompt = examples[\"prompt\"]\n",
    "    chosen = examples[\"chosen\"]\n",
    "    rejected = examples[\"rejected\"]\n",
    "    \n",
    "    def generate_input(_text,_summary):\n",
    "        return \" \".join([_text,_summary])\n",
    "\n",
    "    chosen_inputs = generate_input(prompt, chosen) #[generate_input(_text,_summary) for _text,_summary in zip(prompt,chosen)]\n",
    "    rejected_inputs = generate_input(prompt, rejected) #[generate_input(_text,_summary) for _text,_summary in zip(prompt,rejected)]\n",
    "    \n",
    "    chosen_input_len = len(tokenizer(chosen_inputs, truncation=False).input_ids)\n",
    "    rejected_input_len = len(tokenizer(rejected_inputs, truncation=False).input_ids)\n",
    "    return (chosen_input_len < tokenizer.model_max_length) and (rejected_input_len < tokenizer.model_max_length)\n",
    "\n",
    "filtered_dataset = dpo_dataset.filter(filter_function)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8a57518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "\n",
    "dataset_config = \"mimic-cxr\"\n",
    "model_name = \"biogpt\"\n",
    "save_path: str=\"/nfs/turbo/umms-vgvinodv/models/finetuned-checkpoints/radsum\"\n",
    "save_path = f\"{save_path}/{model_name}-dpo-{dataset_config}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=16,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs = 1,\n",
    "    max_steps =-1,\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 1,\n",
    "    logging_strategy ='steps',\n",
    "    logging_steps=20,\n",
    "    learning_rate=5.0e-7,\n",
    "    output_dir=save_path,\n",
    "    remove_unused_columns=False,\n",
    "    run_name=\"dpo_biogpt\",\n",
    "    overwrite_output_dir = True,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=training_args,\n",
    "    beta=0.1,\n",
    "    train_dataset=dpo_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_prompt_length=512,\n",
    "    max_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a6975de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 15:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.698000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=0.6975356737772623, metrics={'train_runtime': 942.7424, 'train_samples_per_second': 9.774, 'train_steps_per_second': 0.038, 'total_flos': 0.0, 'train_loss': 0.6975356737772623, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d93369a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input:\n",
      "Lateral view somewhat limited due to overlying motion artifact. The lungs are low in volume.  There is no focal airspace consolidation to suggest pneumonia.  A 1.2-cm calcified granuloma just below the medial aspect of the right hemidiaphragm is unchanged from prior study.  No pleural effusions or pulmonary edema. There is no pneumothorax.  The inferior sternotomy wire is fractured but unchanged. Surgical clips and vascular markers in the thorax are related to prior CABG surgery.The main impression based on the given FINDINGS section of the chest X-ray report are:\n",
      "\n",
      "---\n",
      "Generated Output:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varu/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No acute cardiopulmonary process.\n",
      "\n",
      "---\n",
      "Ground Truth:\n",
      "No evidence of acute cardiopulmonary process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "findings_example = f\"\"\"Lateral view somewhat limited due to overlying motion artifact. The lungs are low in volume.  There is no focal airspace consolidation to suggest pneumonia.  A 1.2-cm calcified granuloma just below the medial aspect of the right hemidiaphragm is unchanged from prior study.  No pleural effusions or pulmonary edema. There is no pneumothorax.  The inferior sternotomy wire is fractured but unchanged. Surgical clips and vascular markers in the thorax are related to prior CABG surgery.\"\"\"\n",
    "eval_prompt = findings_example + prompt\n",
    "print(f\"Model Input:\\n{eval_prompt}\\n\")\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"---\\nGenerated Output:\\n\")\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_length=256)[0], skip_special_tokens=True).split(\":\")[-1].strip())\n",
    "    \n",
    "ground_truth_summary=\"\"\"\n",
    "---\n",
    "Ground Truth:\n",
    "No evidence of acute cardiopulmonary process.\n",
    "\"\"\"    \n",
    "print(ground_truth_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c2dd7",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daa99dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from radgraph import F1RadGraph\n",
    "from f1chexbert import F1CheXbert\n",
    "import datasets\n",
    "from pathlib import Path\n",
    "\n",
    "def build_test_dataset(dataset_config, tokenizer, split=\"test\"):\n",
    "    data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "    findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "    impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "\n",
    "\n",
    "    findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "    impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "\n",
    "    dataset = datasets.Dataset.from_dict({\"text\":findings,\"summary\":impression}) \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def generate_summary(sample):\n",
    "    texts = sample[\"text\"]\n",
    "    summaries = sample[\"summary\"]\n",
    "    prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "\n",
    "    def generate_input(_text):\n",
    "        return \" \".join([_text,prompt])\n",
    "\n",
    "    inputs = generate_input(texts) \n",
    "    model_input = tokenizer(inputs, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        response = tokenizer.decode(model.generate(**model_input, max_new_tokens=512)[0], skip_special_tokens=True)\n",
    "    \n",
    "    formatted_response = response.split(\":\")[-1].strip()\n",
    "    return {\n",
    "        \"text\": inputs,\n",
    "        \"summary\":summaries,\n",
    "        \"pred\": formatted_response\n",
    "    }\n",
    "\n",
    "def process_impression(impression):\n",
    "    impression = impression.lower()\n",
    "    return ' '.join(wordpunct_tokenize(impression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947e28bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 1624\n"
     ]
    }
   ],
   "source": [
    "mini_test = False\n",
    "\n",
    "test_dataset = build_test_dataset('mimic-cxr',tokenizer,'test')\n",
    "\n",
    "if mini_test:\n",
    "    num_samples = int(0.25*len(test_dataset))\n",
    "    test_dataset = test_dataset.select(range(num_samples))\n",
    "\n",
    "print(f'Number of test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7691905e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_summary at 0x14c11efafe20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d9e2e2d9094fe6adcbec649adec918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE:\n",
      "{'rouge1': 46.6064, 'rouge2': 31.5597, 'rougeL': 42.8901, 'rougeLsum': 42.88}\n",
      "\n",
      "F1RadGraph:\n",
      "40.70844768219216\n",
      "\n",
      "F1CheXbert:\n",
      "0.7150368033648792\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "results = test_dataset.map(generate_summary, remove_columns=list(test_dataset.features))\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "pred_str = list(map(process_impression,pred_str))\n",
    "label_str = results[\"summary\"]\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "###################################\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "res = {key: value.mid.fmeasure * 100 for key, value in rouge_output.items()}\n",
    "print('ROUGE:')\n",
    "print({k: round(v, 4) for k, v in res.items()})\n",
    "\n",
    "#################################\n",
    "f1radgraph = F1RadGraph(reward_level=\"partial\")\n",
    "score = f1radgraph(hyps=pred_str,refs=label_str)[0]\n",
    "print(\"\\nF1RadGraph:\")\n",
    "print(score*100)\n",
    "\n",
    "#################################\n",
    "f1chexbert = F1CheXbert(device=\"cuda\")\n",
    "accuracy, accuracy_not_averaged, class_report, class_report_5 = f1chexbert(\n",
    "    hyps=pred_str,\n",
    "    refs=label_str)\n",
    "print(\"\\nF1CheXbert:\")\n",
    "print(class_report_5[\"micro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9427a6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hidden test samples: 1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da322f60f484bcb9e0c4c1f1a9d17ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE:\n",
      "{'rouge1': 34.9319, 'rouge2': 19.483, 'rougeL': 29.8388, 'rougeLsum': 29.8586}\n",
      "\n",
      "F1RadGraph:\n",
      "13.099944417229642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1CheXbert:\n",
      "0.6434262948207171\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "hidden_test_dataset = build_test_dataset('mimic-cxr',tokenizer,'test.hidden')\n",
    "\n",
    "print(f'Number of Hidden test samples: {len(hidden_test_dataset)}\\n')\n",
    "\n",
    "results_hidden_test = hidden_test_dataset.map(generate_summary, remove_columns=list(hidden_test_dataset.features))\n",
    "\n",
    "pred_str = results_hidden_test[\"pred\"]\n",
    "pred_str = list(map(process_impression,pred_str))\n",
    "label_str = results_hidden_test[\"summary\"]\n",
    "\n",
    "###################################\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "res = {key: value.mid.fmeasure * 100 for key, value in rouge_output.items()}\n",
    "print('ROUGE:')\n",
    "print({k: round(v, 4) for k, v in res.items()})\n",
    "\n",
    "#################################\n",
    "f1radgraph = F1RadGraph(reward_level=\"partial\")\n",
    "score = f1radgraph(hyps=pred_str,refs=label_str)[0]\n",
    "print(\"\\nF1RadGraph:\")\n",
    "print(score*100)\n",
    "\n",
    "#################################\n",
    "f1chexbert = F1CheXbert(device=\"cuda\")\n",
    "accuracy, accuracy_not_averaged, class_report, class_report_5 = f1chexbert(\n",
    "    hyps=pred_str,\n",
    "    refs=label_str)\n",
    "print(\"\\nF1CheXbert:\")\n",
    "print(class_report_5[\"micro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0642bfd7",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b1f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from radgraph import F1RadGraph\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "f1radgraph = F1RadGraph(reward_level=\"partial\")\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 512,\n",
    "}\n",
    "\n",
    "bs = 32\n",
    "game_data = dict()\n",
    "filtered_dataset = dpo_dataset\n",
    "filtered_dataset.set_format(\"pandas\")\n",
    "df_batch = filtered_dataset[:].sample(bs)\n",
    "\n",
    "game_data[\"GT\"] = df_batch[\"chosen\"].tolist()\n",
    "\n",
    "query_tensors = tokenizer(df_batch[\"prompt\"].tolist()).input_ids\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    output = model_ref.generate(torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(\"cuda\"), **generation_kwargs).squeeze()\n",
    "    response_tensors_ref.append(output)\n",
    "    output = model.generate(torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(\"cuda\"), **generation_kwargs).squeeze()\n",
    "    response_tensors.append(output)\n",
    "\n",
    "#### decode responses\n",
    "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i],skip_special_tokens=True).split(\":\")[-1].strip() for i in range(bs)]\n",
    "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i],skip_special_tokens=True).split(\":\")[-1].strip() for i in range(bs)]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "game_data[\"F1RadGraph rewards (before)\"] = f1radgraph(hyps=game_data[\"response (before)\"], refs=game_data[\"GT\"])[1]\n",
    "game_data[\"F1RadGraph rewards (after)\"] = f1radgraph(hyps=game_data[\"response (after)\"], refs=game_data[\"GT\"])[1]\n",
    "\n",
    "game_data[\"rougeL rewards (before)\"] = rouge.compute(predictions=game_data[\"response (before)\"], references=game_data[\"GT\"], rouge_types=['rougeL'],  use_aggregator=False)['rougeL']\n",
    "game_data[\"rougeL rewards (after)\"] = rouge.compute(predictions=game_data[\"response (after)\"], references=game_data[\"GT\"], rouge_types=['rougeL'],  use_aggregator=False)['rougeL']\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e287636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets  import Dataset\n",
    "\n",
    "result = Dataset.from_pandas(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132dc901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d416fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9683f6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BioGptForCausalLM\n",
    "\n",
    "checkpoint = \"/nfs/turbo/umms-vgvinodv/models/finetuned-checkpoints/radsum/biogpt-dpo-mimic-cxr/checkpoint-30290\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = BioGptForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade86a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_tokens\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c277423",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "findings_example = f\"\"\"Lateral view somewhat limited due to overlying motion artifact. The lungs are low in volume.  There is no focal airspace consolidation to suggest pneumonia.  A 1.2-cm calcified granuloma just below the medial aspect of the right hemidiaphragm is unchanged from prior study.  No pleural effusions or pulmonary edema. There is no pneumothorax.  The inferior sternotomy wire is fractured but unchanged. Surgical clips and vascular markers in the thorax are related to prior CABG surgery.\"\"\"\n",
    "eval_prompt = findings_example + prompt\n",
    "print(f\"Model Input:\\n{eval_prompt}\\n\")\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"---\\nGenerated Output:\\n\")\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_length=256)[0], skip_special_tokens=True).split(\":\")[-1].strip())\n",
    "    \n",
    "ground_truth_summary=\"\"\"\n",
    "---\n",
    "Ground Truth:\n",
    "No evidence of acute cardiopulmonary process.\n",
    "\"\"\"    \n",
    "print(ground_truth_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from radgraph import F1RadGraph\n",
    "from f1chexbert import F1CheXbert\n",
    "import datasets\n",
    "from pathlib import Path\n",
    "\n",
    "def build_test_dataset(dataset_config, tokenizer, split=\"test\"):\n",
    "    data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "    findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "    impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "\n",
    "\n",
    "    findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "    impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "\n",
    "    dataset = datasets.Dataset.from_dict({\"text\":findings,\"summary\":impression}) \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def generate_summary(sample):\n",
    "    texts = sample[\"text\"]\n",
    "    summaries = sample[\"summary\"]\n",
    "    prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "\n",
    "    def generate_input(_text):\n",
    "        return \" \".join([_text,prompt])\n",
    "\n",
    "    inputs = generate_input(texts) \n",
    "    model_input = tokenizer(inputs, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        response = tokenizer.decode(model.generate(**model_input, max_new_tokens=512)[0], skip_special_tokens=True)\n",
    "    \n",
    "    formatted_response = response.split(\":\")[-1].strip()\n",
    "    return {\n",
    "        \"text\": inputs,\n",
    "        \"summary\":summaries,\n",
    "        \"pred\": formatted_response\n",
    "    }\n",
    "\n",
    "def process_impression(impression):\n",
    "    impression = impression.lower()\n",
    "    return ' '.join(wordpunct_tokenize(impression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ab6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = build_test_dataset('mimic-cxr',tokenizer,'test')\n",
    "print(f'Number of test samples: {len(test_dataset)}')\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "results = test_dataset.map(generate_summary, remove_columns=list(test_dataset.features))\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "pred_str = list(map(process_impression,pred_str))\n",
    "label_str = results[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "###################################\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "res = {key: value.mid.fmeasure * 100 for key, value in rouge_output.items()}\n",
    "print('ROUGE:')\n",
    "print({k: round(v, 4) for k, v in res.items()})\n",
    "\n",
    "##################################\n",
    "bertscore = datasets.load_metric(\"bertscore\")\n",
    "bertscore_output = bertscore.compute(predictions=pred_str, references=label_str, lang='en')\n",
    "res = {key: np.asarray(value).mean()*100 for key, value in bertscore_output.items() if key != 'hashcode'}\n",
    "print('BertScore:')\n",
    "print({k: round(v,4) for k, v in res.items()})\n",
    "\n",
    "#################################\n",
    "f1radgraph = F1RadGraph(reward_level=\"partial\")\n",
    "score = f1radgraph(hyps=pred_str,refs=label_str)[0]\n",
    "print(\"\\nF1RadGraph:\")\n",
    "print(score*100)\n",
    "\n",
    "#################################\n",
    "f1chexbert = F1CheXbert(device=\"cuda\")\n",
    "accuracy, accuracy_not_averaged, class_report, class_report_5 = f1chexbert(\n",
    "    hyps=pred_str,\n",
    "    refs=label_str)\n",
    "print(\"\\nF1CheXbert:\")\n",
    "print(class_report_5[\"micro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0edcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
