{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be489770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datasets\n",
    "from datasets import Dataset, Features, Image\n",
    "import os.path\n",
    "\n",
    "#dataset_config = 'mimic-cxr','mimic-iii'  \n",
    "#split = 'train','validate',test\n",
    "def build_dataset(dataset_config, split):\n",
    "    def generate_image_path(line):\n",
    "        return str(Path(data_path).joinpath(dataset_config).joinpath(line.strip().split(',')[0]))\n",
    "    \n",
    "    data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "    \n",
    "    findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "    impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "    image_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.image.tok')\n",
    "\n",
    "\n",
    "    findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "    impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "    image_paths = [generate_image_path(line) for line in open(image_file_path).readlines()]\n",
    "    \n",
    "    dataset = datasets.Dataset.from_dict({\"text\":findings,\"image\":image_paths})\n",
    "    \n",
    "    def check_img_exists(example):\n",
    "        return os.path.isfile(example[\"image\"]) #example[\"image\"].split('/')[10] != 'p10'\n",
    "\n",
    "    dataset = dataset.filter(check_img_exists, num_proc=4)\n",
    "    dataset = dataset.cast_column(\"image\", Image())\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd630594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_dataset = build_dataset(\"mimic-cxr\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90838a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_dataset)\n",
    "print(raw_dataset[0][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GanjinZero/biobart-base\")\n",
    "\n",
    "def tokenize(samples):\n",
    "    input_text = [\"summarize: \"+text for text in samples[\"text\"]]\n",
    "    samples[\"input_ids\"] = tokenizer(input_text)[\"input_ids\"]\n",
    "    return samples\n",
    "\n",
    "train_dataset = raw_dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5084ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "def image_transforms(samples):\n",
    "    samples[\"query\"] = [transform(image.convert(\"RGB\").resize((384,384))) for image in samples[\"image\"]]\n",
    "    return samples\n",
    "\n",
    "#image_text_dataset = raw_dataset.map(image_transforms, remove_columns=[\"image\"], batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_transform(image_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499ae60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b61c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datasets\n",
    "from datasets import Image\n",
    "from torchvision import transforms\n",
    "import os.path\n",
    "\n",
    "#dataset_config = 'mimic-cxr','mimic-iii'  \n",
    "#split = 'train','validate',test\n",
    "def build_dataset(dataset_config, tokenizer, split):\n",
    "    def generate_image_path(line):\n",
    "        return str(Path(data_path).joinpath(dataset_config).joinpath(line.strip().split(',')[0]))\n",
    "    \n",
    "    data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "    \n",
    "    findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "    impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "    image_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.image.tok')\n",
    "\n",
    "\n",
    "    findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "    impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "    image_paths = [generate_image_path(line) for line in open(image_file_path).readlines()]\n",
    "    \n",
    "    dataset = datasets.Dataset.from_dict({\"text\":findings,\"query\":image_paths})\n",
    "    \n",
    "    def check_img_exists(example):\n",
    "        return os.path.isfile(example[\"query\"]) \n",
    "\n",
    "    dataset = dataset.filter(check_img_exists, num_proc=4)\n",
    "    dataset = dataset.cast_column(\"query\", Image())\n",
    "    \n",
    "    def tokenize(samples):\n",
    "        input_text = [\" \".join(['summarize:',text]) for text in samples[\"text\"]]\n",
    "        samples[\"input_ids\"] = tokenizer(input_text).input_ids\n",
    "        #samples[\"input_ids\"] = tokenizer.encode(input_text, padding=True, return_tensors=\"pt\")#.input_ids \n",
    "        return samples\n",
    "    \n",
    "    dataset = dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "    dataset.set_format(\"pt\", columns=['input_ids'], output_all_columns=True)\n",
    "    \n",
    "    normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "        #transforms.Resize(384)\n",
    "    ])\n",
    "    \n",
    "    text_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    def image_transforms(samples):\n",
    "        samples[\"query\"] = [transform(image.convert(\"RGB\").resize((384,384))) for image in samples[\"query\"]]\n",
    "        samples[\"input_ids\"] = \n",
    "        return samples\n",
    "    #dataset = dataset.map(image_transforms, batched=True)\n",
    "    #dataset.set_format(type=\"torch\")\n",
    "    \n",
    "    \n",
    "    dataset.set_transform(image_transforms)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6cf2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GanjinZero/biobart-base\")\n",
    "\n",
    "train_data = build_dataset(\"mimic-cxr\", tokenizer, \"train\")\n",
    "#train_data.set_format(type=\"torch\")\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70776d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[:2][\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0960199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0][\"query\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f149dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_data[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ed0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GanjinZero/biobart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac916f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\"no acute process\"]*2\n",
    "print(len(input_text[1]))\n",
    "text = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(text.input_ids)\n",
    "print(text.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datasets\n",
    "from datasets import Image\n",
    "from torchvision import transforms\n",
    "import os.path\n",
    "\n",
    "def generate_image_path(line):\n",
    "    return str(Path(data_path).joinpath(dataset_config).joinpath(line.strip().split(',')[0]))\n",
    "\n",
    "dataset_config = \"mimic-cxr\"\n",
    "split=\"train\"\n",
    "data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "\n",
    "findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "image_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.image.tok')\n",
    "\n",
    "\n",
    "findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "image_paths = [generate_image_path(line) for line in open(image_file_path).readlines()]\n",
    "\n",
    "dataset = datasets.Dataset.from_dict({\"text\":findings,\"query\":image_paths})\n",
    "\n",
    "def check_img_exists(example):\n",
    "    return os.path.isfile(example[\"query\"])\n",
    "\n",
    "dataset = dataset.filter(check_img_exists, num_proc=4)\n",
    "dataset = dataset.cast_column(\"query\", Image())\n",
    "\n",
    "def tokenize(samples):\n",
    "    input_text = [\" \".join(['summarize:',text]) for text in samples[\"text\"]]\n",
    "    samples[\"input_ids\"] = tokenizer(input_text).input_ids\n",
    "    return samples\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8be257",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea128e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "\n",
    "def image_transforms(samples):\n",
    "    samples[\"query\"] = [image.convert(\"RGB\").resize((384,384)) for image in samples[\"query\"]]\n",
    "    return samples\n",
    "dataset = dataset.map(image_transforms, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eca3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10678012",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"ppo_dataset.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3db002",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(raw_dataset['input_ids'][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ee9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41deaeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.set_format(type=\"torch\", columns=['input_ids'],output_all_columns=True)\n",
    "print(raw_dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_dataset[0][\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed851a9",
   "metadata": {},
   "source": [
    "# create_ppo_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9a69a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42323426c03b44528ead22c04bacf699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/125417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'No focal consolidation is seen. There is no pleural effusion or pneumothorax. The cardiac and mediastinal silhouettes are unremarkable.', 'query': <PIL.JpegImagePlugin.JpegImageFile image mode=L size=512x615 at 0x15337A0BCAC0>}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import datasets\n",
    "from datasets import Image\n",
    "from torchvision import transforms\n",
    "import os.path\n",
    "\n",
    "\n",
    "def generate_image_path(line):\n",
    "    return str(Path(data_path).joinpath(dataset_config).joinpath(line.strip().split(',')[0]))\n",
    "\n",
    "dataset_config = \"mimic-cxr\"\n",
    "split=\"train\"\n",
    "data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "\n",
    "findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "image_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.image.tok')\n",
    "\n",
    "\n",
    "findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "image_paths = [generate_image_path(line) for line in open(image_file_path).readlines()]\n",
    "\n",
    "dataset = datasets.Dataset.from_dict({\"text\":findings,\"query\":image_paths})\n",
    "\n",
    "def check_img_exists(example):\n",
    "    return os.path.isfile(example[\"query\"])\n",
    "\n",
    "dataset = dataset.filter(check_img_exists, num_proc=4)\n",
    "dataset = dataset.cast_column(\"query\", Image())\n",
    "\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b441d6bc9d2647599456bd168f8559a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import ImageFeatureExtractionMixin\n",
    "\n",
    "feature_extractor = ImageFeatureExtractionMixin()\n",
    "\n",
    "def to_pixels(image):\n",
    "    image = feature_extractor.resize(image, size=384)\n",
    "    image = feature_extractor.convert_rgb(image)\n",
    "    #image = feature_extractor.normalize(image, mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "    #image = feature_extractor.to_numpy_array(image)\n",
    "    return image\n",
    "\n",
    "def process(examples):\n",
    "    examples[\"pixel_values\"] = [to_pixels(image) for image in examples[\"query\"]]\n",
    "    return examples\n",
    "\n",
    "#features = Features({\"pixel_values\":})\n",
    "\n",
    "prep_dataset = dataset.map(process, batched=True, batch_size=256)\n",
    "print(prep_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ba2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ImageFeatureExtractionMixin\n",
    "\n",
    "feature_extractor = ImageFeatureExtractionMixin()\n",
    "\n",
    "#convert_rgb\n",
    "def to_pixels(image):\n",
    "    image = feature_extractor.resize(image, size=384)\n",
    "    image = feature_extractor.convert_rgb(image)\n",
    "    image = feature_extractor.normalize(image, mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "    image = feature_extractor.to_numpy_array(image)\n",
    "    return image\n",
    "\n",
    "def process(examples):\n",
    "    examples[\"pixel_values\"] = [to_pixels(image) for image in examples[\"query\"]]\n",
    "    return examples\n",
    "\n",
    "#features = Features({\"pixel_values\":})\n",
    "\n",
    "prep_dataset = dataset.map(process, batched=True, batch_size=256)\n",
    "print(prep_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d450cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "\n",
    "def image_transforms(samples):\n",
    "    samples[\"query\"] = [transform(image.convert(\"RGB\").resize((384,384))) for image in samples[\"query\"]]\n",
    "    return samples\n",
    "dataset = dataset.map(image_transforms, batched=True)\n",
    "\n",
    "dataset.save_to_disk(\"ppo_dataset.hf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
