{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1faa48",
   "metadata": {},
   "source": [
    "# Llama 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a52c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2a80d566204f1a8da87236f0442e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import transformers\n",
    "\n",
    "checkpoint = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# load the base model in 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,       \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd86629",
   "metadata": {},
   "source": [
    "### Check base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05796080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input:\n",
      "Lateral view somewhat limited due to overlying motion artifact. The lungs are low in volume.  There is no focal airspace consolidation to suggest pneumonia.  A 1.2-cm calcified granuloma just below the medial aspect of the right hemidiaphragm is unchanged from prior study.  No pleural effusions or pulmonary edema. There is no pneumothorax.  The inferior sternotomy wire is fractured but unchanged. Surgical clips and vascular markers in the thorax are related to prior CABG surgery.The main impression based on the given FINDINGS section of the chest X-ray report are:\n",
      "\n",
      "---\n",
      "Generated Output:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varu/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. No pneumothorax 2. No focal airspace consolidation to suggest pneumonia 3. No pleural effusions 4. No pulmonary edema 5. No focal calcified granuloma 6. No surgical clips\n",
      "\n",
      "---\n",
      "Ground Truth:\n",
      "No evidence of acute cardiopulmonary process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "findings_example = f\"\"\"Lateral view somewhat limited due to overlying motion artifact. The lungs are low in volume.  There is no focal airspace consolidation to suggest pneumonia.  A 1.2-cm calcified granuloma just below the medial aspect of the right hemidiaphragm is unchanged from prior study.  No pleural effusions or pulmonary edema. There is no pneumothorax.  The inferior sternotomy wire is fractured but unchanged. Surgical clips and vascular markers in the thorax are related to prior CABG surgery.\"\"\"\n",
    "eval_prompt = findings_example + prompt\n",
    "print(f\"Model Input:\\n{eval_prompt}\\n\")\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"---\\nGenerated Output:\\n\")\n",
    "    print(tokenizer.decode(base_model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True).split(\":\")[-1].strip())\n",
    "    \n",
    "ground_truth_summary=\"\"\"\n",
    "---\n",
    "Ground Truth:\n",
    "No evidence of acute cardiopulmonary process.\n",
    "\"\"\"    \n",
    "print(ground_truth_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a84121",
   "metadata": {},
   "source": [
    "### Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef1ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "\n",
    "class Concatenator(object):\n",
    "    def __init__(self, chunk_size=1024):\n",
    "        self.chunk_size=chunk_size\n",
    "        self.residual = {\"input_ids\": [], \"attention_mask\": []}\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        concatenated_samples = {\n",
    "            k: v + list(chain(*batch[k])) for k, v in self.residual.items()\n",
    "        }\n",
    "\n",
    "        total_length = len(concatenated_samples[list(concatenated_samples.keys())[0]])\n",
    "\n",
    "        if total_length >= self.chunk_size:\n",
    "            chunk_num = total_length // self.chunk_size\n",
    "            result = {\n",
    "                k: [\n",
    "                    v[i : i + self.chunk_size]\n",
    "                    for i in range(0, chunk_num * self.chunk_size, self.chunk_size)\n",
    "                ]\n",
    "                for k, v in concatenated_samples.items()\n",
    "            }\n",
    "            self.residual = {\n",
    "                k: v[(chunk_num * self.chunk_size) :]\n",
    "                for k, v in concatenated_samples.items()\n",
    "            }\n",
    "        else:\n",
    "            result = concatenated_samples\n",
    "            self.residual = {k: [] for k in concatenated_samples.keys()}\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "    \n",
    "#dataset_config = 'mimic-cxr','mimic-iii'  \n",
    "#split = 'train','validate',test\n",
    "def build_dataset(dataset_config, tokenizer, split):\n",
    "    data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "    findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "    impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "\n",
    "\n",
    "    findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "    impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "\n",
    "    dataset = datasets.Dataset.from_dict({\"text\":findings,\"summary\":impression}) \n",
    "    \n",
    "   \n",
    "    #prompt = (\n",
    "    #    f\"FINDINGS:{{text}}\\n\\n The main impression based on the given FINDINGS section of the chest X-ray report are as follows\\n\\nIMPRESSION:{{summary}}{{eos_token}}\"\n",
    "    #)\n",
    "    prompt = (\n",
    "        f\"{{text}} The main impression based on the given FINDINGS section of the chest X-ray report are: {{summary}}{{eos_token}}\"\n",
    "    )\n",
    "\n",
    "    def apply_prompt_template(sample):\n",
    "        return {\n",
    "            \"text\": prompt.format(\n",
    "                text=sample[\"text\"],\n",
    "                summary=sample[\"summary\"],\n",
    "                eos_token=tokenizer.eos_token,\n",
    "            )\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(apply_prompt_template, remove_columns=list(dataset.features))\n",
    "    dataset = dataset.map(\n",
    "        lambda sample: tokenizer(sample[\"text\"]),\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=list(dataset.features),\n",
    "    ).map(Concatenator(), batched=True, num_proc=4)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af6c269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c71fc3ab9b48bc887c90db9da2627e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cc4e118d264f8ab1f9248a523fbc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/125417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc772c38fe14424a906f673b8dfa104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/125417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8848ab0a9efc4df08a12f861e8f45e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5eb28fcd394f3a88b59fc23ea1fd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e990ce3ae2df4aea83172efecaf6f068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 16519\n",
      "Number of training samples: 257\n"
     ]
    }
   ],
   "source": [
    "dataset_config: str=\"mimic-cxr\"\n",
    "    \n",
    "train_dataset = build_dataset(dataset_config, tokenizer, 'train')\n",
    "eval_dataset = build_dataset(dataset_config, tokenizer, \"test\")\n",
    "\n",
    "print(f'Number of training samples: {len(train_dataset)}')\n",
    "print(f'Number of training samples: {len(eval_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b124fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0a496",
   "metadata": {},
   "source": [
    "### Prepare PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed927107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_kbit_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "\n",
    "    # prepare int-8/int-4 model for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c224e8b",
   "metadata": {},
   "source": [
    "### Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd19d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "batch_size = 16\n",
    "num_train_epochs = 1\n",
    "save_path: str=\"/nfs/turbo/umms-vgvinodv/models/finetuned-checkpoints/radsum\"\n",
    "save_path = f\"{save_path}/{model_name}-{dataset_config}\"\n",
    "\n",
    "# Training Args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    overwrite_output_dir = True,\n",
    "    fp16=True,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "#'gradient_accumulation_steps': 4,\n",
    "#'gradient_checkpointing': True,\n",
    "\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6282e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cca5e7",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc335d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from radgraph import F1RadGraph\n",
    "from f1chexbert import F1CheXbert\n",
    "import datasets\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def build_test_dataset(dataset_config, tokenizer, split=\"test\"):\n",
    "    data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "    findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "    impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "\n",
    "\n",
    "    findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "    impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "\n",
    "    dataset = datasets.Dataset.from_dict({\"text\":findings,\"summary\":impression}) \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def generate_summary(sample):\n",
    "    texts = sample[\"text\"]\n",
    "    summaries = sample[\"summary\"]\n",
    "    prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "\n",
    "    def generate_input(_text):\n",
    "        return \" \".join([_text,prompt])\n",
    "\n",
    "    inputs = generate_input(texts) \n",
    "    model_input = tokenizer(inputs, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        response = tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True)\n",
    "    \n",
    "    formatted_response = response.split(\":\")[-1].strip()\n",
    "    return {\n",
    "        \"text\": inputs,\n",
    "        \"summary\":summaries,\n",
    "        \"pred\": formatted_response\n",
    "    }\n",
    "\n",
    "def process_impression(impression):\n",
    "    impression = impression.lower()\n",
    "    return ' '.join(wordpunct_tokenize(impression))\n",
    "\n",
    "def compute_metrics(pred_str, label_str):\n",
    "    ###################################\n",
    "    rouge = datasets.load_metric(\"rouge\")\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    res = {key: value.mid.fmeasure * 100 for key, value in rouge_output.items()}\n",
    "    print('ROUGE:')\n",
    "    print({k: round(v, 4) for k, v in res.items()})\n",
    "\n",
    "    ##################################\n",
    "    bertscore = datasets.load_metric(\"bertscore\")\n",
    "    bertscore_output = bertscore.compute(predictions=pred_str, references=label_str, lang='en')\n",
    "    res = {key: np.asarray(value).mean()*100 for key, value in bertscore_output.items() if key != 'hashcode'}\n",
    "    print('BertScore:')\n",
    "    print({k: round(v,4) for k, v in res.items()})\n",
    "\n",
    "    #################################\n",
    "    f1radgraph = F1RadGraph(reward_level=\"partial\")\n",
    "    score = f1radgraph(hyps=pred_str,refs=label_str)[0]\n",
    "    print(\"\\nF1RadGraph:\")\n",
    "    print(score*100)\n",
    "\n",
    "    #################################\n",
    "    f1chexbert = F1CheXbert(device=\"cuda\")\n",
    "    accuracy, accuracy_not_averaged, class_report, class_report_5 = f1chexbert(\n",
    "        hyps=pred_str,\n",
    "        refs=label_str)\n",
    "    print(\"\\nF1CheXbert:\")\n",
    "    print(class_report_5[\"micro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151cbbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 1624\n"
     ]
    }
   ],
   "source": [
    "test_dataset = build_test_dataset(dataset_config, tokenizer, 'test')\n",
    "print(f'Number of test samples: {len(test_dataset)}')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "results = test_dataset.map(generate_summary, remove_columns=list(test_dataset.features))\n",
    "pred_str = results[\"pred\"]\n",
    "pred_str = list(map(process_impression,pred_str))\n",
    "label_str = results[\"summary\"]\n",
    "\n",
    "compute_metrics(pred_str, label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = build_test_dataset(dataset_config, tokenizer, 'test.hidden')\n",
    "print(f'Number of test samples: {len(test_dataset)}')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "results = test_dataset.map(generate_summary, remove_columns=list(test_dataset.features))\n",
    "pred_str = results[\"pred\"]\n",
    "pred_str = list(map(process_impression,pred_str))\n",
    "label_str = results[\"summary\"]\n",
    "\n",
    "compute_metrics(pred_str, label_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f5b1bc",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "model_name = \"chaoyi-wu/PMC_LLAMA_7B\"#'chaoyi-wu/MedLLaMA_13B'\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
    "sentence = 'Hello, doctor' \n",
    "batch = tokenizer(\n",
    "            sentence,\n",
    "            return_tensors=\"pt\", \n",
    "            add_special_tokens=False\n",
    "        )\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(inputs = batch[\"input_ids\"], max_length=200, do_sample=True, top_k=50)\n",
    "    print('model predict: ',tokenizer.decode(generated[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generated = model.generate(inputs = batch[\"input_ids\"].to('cuda'), max_length=100, do_sample=True, top_k=50)\n",
    "    print('model predict: ',tokenizer.decode(generated[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cddf1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"chaoyi-wu/PMC_LLAMA_7B\"\n",
    "#model_name = \"chaoyi-wu/MedLLaMA_13B\"\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# load the base model in 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,       \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf9a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f81a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"chaoyi-wu/PMC_LLAMA_7B\"#'chaoyi-wu/MedLLaMA_13B'\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name)\n",
    "base_model = transformers.LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6eca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89248c28",
   "metadata": {},
   "source": [
    "### Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "\n",
    "class Concatenator(object):\n",
    "    def __init__(self, chunk_size=1024):\n",
    "        self.chunk_size=chunk_size\n",
    "        self.residual = {\"input_ids\": [], \"attention_mask\": []}\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        concatenated_samples = {\n",
    "            k: v + list(chain(*batch[k])) for k, v in self.residual.items()\n",
    "        }\n",
    "\n",
    "        total_length = len(concatenated_samples[list(concatenated_samples.keys())[0]])\n",
    "\n",
    "        if total_length >= self.chunk_size:\n",
    "            chunk_num = total_length // self.chunk_size\n",
    "            result = {\n",
    "                k: [\n",
    "                    v[i : i + self.chunk_size]\n",
    "                    for i in range(0, chunk_num * self.chunk_size, self.chunk_size)\n",
    "                ]\n",
    "                for k, v in concatenated_samples.items()\n",
    "            }\n",
    "            self.residual = {\n",
    "                k: v[(chunk_num * self.chunk_size) :]\n",
    "                for k, v in concatenated_samples.items()\n",
    "            }\n",
    "        else:\n",
    "            result = concatenated_samples\n",
    "            self.residual = {k: [] for k in concatenated_samples.keys()}\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "    \n",
    "#dataset_config = 'mimic-cxr','mimic-iii'  \n",
    "#split = 'train','validate',test\n",
    "def build_dataset(dataset_config, tokenizer, split):\n",
    "    data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "    findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "    impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "\n",
    "\n",
    "    findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "    impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "\n",
    "    dataset = datasets.Dataset.from_dict({\"text\":findings,\"summary\":impression}) \n",
    "    \n",
    "   \n",
    "    prompt = (\n",
    "        f\"FINDINGS:{{text}}\\n\\n The main impression based on the given FINDINGS section of the chest X-ray report are as follows\\n\\nIMPRESSION:{{summary}}{{eos_token}}\"\n",
    "    )\n",
    "\n",
    "    def apply_prompt_template(sample):\n",
    "        return {\n",
    "            \"text\": prompt.format(\n",
    "                text=sample[\"text\"],\n",
    "                summary=sample[\"summary\"],\n",
    "                eos_token=tokenizer.eos_token,\n",
    "            )\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(apply_prompt_template, remove_columns=list(dataset.features))\n",
    "    dataset = dataset.map(\n",
    "        lambda sample: tokenizer(sample[\"text\"]),\n",
    "        batched=True,\n",
    "        remove_columns=list(dataset.features),\n",
    "    ).map(Concatenator(), batched=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a20b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "import itertools\n",
    "#dataset_config = 'mimic-cxr','mimic-iii'  \n",
    "#split = 'train','validate',test\n",
    "\n",
    "def build_dataset(dataset_config, tokenizer, split):\n",
    "    data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "    findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "    impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "\n",
    "\n",
    "    findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "    impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "\n",
    "    dataset = datasets.Dataset.from_dict({\"text\":findings,\"summary\":impression}) \n",
    "    \n",
    "   \n",
    "    prompt = (\n",
    "        f\"{{text}} The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "    )\n",
    "    def apply_prompt_template(sample):\n",
    "        return {\n",
    "            \"text\": prompt.format(text=sample[\"text\"]),\n",
    "            \"summary\": sample[\"summary\"],\n",
    "        }\n",
    "        \n",
    "    dataset = dataset.map(apply_prompt_template, num_proc=4, remove_columns=list(dataset.features))\n",
    "    \n",
    "    def tokenize_add_label(samples):\n",
    "        texts = samples[\"text\"]\n",
    "        summaries = samples[\"summary\"]\n",
    "        \n",
    "        prompt_tokens = [tokenizer.encode(tokenizer.bos_token + _text, add_special_tokens=False) for _text in texts]\n",
    "        answer_tokens = [tokenizer.encode(_summary +  tokenizer.eos_token, add_special_tokens=False) for _summary in summaries]\n",
    "        \n",
    "        dialog_tokens = list(itertools.chain.from_iterable(zip(prompt_tokens, answer_tokens)))\n",
    "        \n",
    "        labels_tokens = [[-100] * len(_prompt_tokens) + _answer_tokens for _prompt_tokens,_answer_tokens in zip(prompt_tokens,answer_tokens)]\n",
    "        \n",
    "        combined_tokens = {\n",
    "            \"input_ids\": list(itertools.chain(*(t for t in dialog_tokens))),\n",
    "            \"labels\": list(itertools.chain(*(t for t in labels_tokens))),\n",
    "        }\n",
    "        \n",
    "        return dict(combined_tokens, attention_mask=[1]*len(combined_tokens[\"input_ids\"]))\n",
    "    \n",
    "    dataset = dataset.map(tokenize_add_label, batched=True, num_proc=4, remove_columns=list(dataset.features))\n",
    "\n",
    "    ########\n",
    "    def preprocess_function(samples):\n",
    "        texts = samples[\"text\"]\n",
    "        summaries = samples[\"summary\"]\n",
    "        prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "        \n",
    "        def generate_input(_text,_summary):\n",
    "            return \" \".join([_text,prompt,_summary])\n",
    "\n",
    "        inputs = [generate_input(_text,_summary) for _text,_summary in zip(texts,summaries)]\n",
    "        model_inputs = tokenizer(inputs)\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "    dataset = dataset.map(preprocess_function, batched=True, num_proc=4, remove_columns=list(dataset.features))\n",
    "\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0865a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_source: str=\"mimic-cxr\"\n",
    "    \n",
    "train_dataset = build_dataset(dataset_source, tokenizer, 'train')\n",
    "print(f'Number of training samples: {len(train_dataset)}')\n",
    "\n",
    "eval_dataset = build_dataset(dataset_source,tokenizer,\"test\")\n",
    "print(f'Number of training samples: {len(eval_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6912693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_samples = int(0.01*len(train_dataset))\n",
    "#train_dataset = train_dataset.select(range(num_samples))\n",
    "#print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc336e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67913ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_kbit_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "\n",
    "    # prepare int-8/int-4 model for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'chaoyi-wu/MedLLaMA_13B'\n",
    "name = model_name.split(\"/\")[-1]\n",
    "output_dir = f\"finetuned-checkpoints/{name}-{dataset_source}\"\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_train_epochs': 1,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'per_device_train_batch_size': 8,\n",
    "    'gradient_checkpointing': True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b162fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True).to(\"cuda\")\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True).to(\"cuda\")\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Note that other metrics may not have a `use_aggregator` parameter\n",
    "    # and thus will return a list, computing a metric for each sentence.\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # Extract a few results\n",
    "    #result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    #prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    #result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    #return {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf80a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logits = p.predictions\n",
    "#labels = p.label_ids\n",
    "#probabilities = softmax(logits, axis=-1)\n",
    "#loss = log_loss(labels.flatten(), probabilities.reshape(-1, probabilities.shape[-1]), labels=[i for i in range(logits.shape[-1])])\n",
    "#perplexity = np.exp(loss)\n",
    "#return {\"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2171ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=True,  # Use FP16 if available\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",#\"epoch\",\n",
    "    optim=\"adamw_torch\",\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    #compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6cc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The main impression based on the given FINDINGS section of the chest X-ray report are:\"\n",
    "findings_example = f\"\"\"Lateral view somewhat limited due to overlying motion artifact. The lungs are low in volume.  There is no focal airspace consolidation to suggest pneumonia.  A 1.2-cm calcified granuloma just below the medial aspect of the right hemidiaphragm is unchanged from prior study.  No pleural effusions or pulmonary edema. There is no pneumothorax.  The inferior sternotomy wire is fractured but unchanged. Surgical clips and vascular markers in the thorax are related to prior CABG surgery.\"\"\"\n",
    "eval_prompt = findings_example + prompt\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"---\\nGenerated Output:\\n\")\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True)[len(eval_prompt):].strip())\n",
    "    \n",
    "ground_truth_summary=\"\"\"\n",
    "---\n",
    "Ground Truth:\n",
    "No evidence of acute cardiopulmonary process.\n",
    "\"\"\"    \n",
    "print(ground_truth_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97566c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
