{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e00db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from trl import AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cfe23e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"./finetuned-checkpoints/biobart-base--mimic-cxr/checkpoint-19600\",\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=None,\n",
    "    mini_batch_size=4,\n",
    "    batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    early_stopping=False,\n",
    "    target_kl=6.0,\n",
    "    kl_penalty=\"kl\",\n",
    "    seed=42,\n",
    "    use_score_scaling=False,\n",
    "    use_score_norm=False,\n",
    "    score_clip=None,\n",
    ")\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685791c",
   "metadata": {},
   "source": [
    "## Load data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87a08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b736543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datasets\n",
    "from datasets import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "#dataset_config = 'mimic-cxr','mimic-iii'  \n",
    "#split = 'train','validate',test\n",
    "def build_dataset(dataset_config, tokenizer, split):\n",
    "    def generate_image_path(line):\n",
    "        return str(Path(data_path).joinpath(dataset_config).joinpath(line.strip().split(',')[0]))\n",
    "    \n",
    "    data_path = '/nfs/turbo/umms-vgvinodv/data/bioNLP23-Task-1B/data/'\n",
    "    \n",
    "    findings_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.findings.tok')\n",
    "    impression_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.impression.tok')\n",
    "    image_file_path = Path(data_path).joinpath(dataset_config).joinpath(split+'.image.tok')\n",
    "\n",
    "\n",
    "    findings = [line.strip() for line in open(findings_file_path).readlines()]\n",
    "    impression = [line.strip() for line in open(impression_file_path).readlines()]\n",
    "    image_paths = [generate_image_path(line) for line in open(image_file_path).readlines()]\n",
    "    \n",
    "    dataset = datasets.Dataset.from_dict({\"text\":findings,\"image\":image_paths})\n",
    "    \n",
    "    def check_img_exists(example):\n",
    "        return example[\"image\"].split('/')[10] != 'p10'\n",
    "\n",
    "    dataset = dataset.filter(check_img_exists)\n",
    "    dataset = dataset.cast_column(\"image\", Image())\n",
    "    \n",
    "    def tokenize(samples):\n",
    "        input_text = [\"summarize: \"+text for text in samples[\"text\"]]\n",
    "        samples[\"input_ids\"] = tokenizer(input_text)[\"input_ids\"]\n",
    "        return samples\n",
    "    \n",
    "    dataset = dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "    \n",
    "    normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    def image_transforms(samples):\n",
    "        samples[\"query\"] = [transform(image.convert(\"RGB\").resize((384,384))) for image in samples[\"image\"]]\n",
    "        return samples\n",
    "    \n",
    "    dataset.set_transform(image_transforms)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3be46ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/125417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/113182 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_config = \"mimic-cxr\"\n",
    "tokenized_train_data = build_dataset(dataset_config,tokenizer,\"train\")\n",
    "tokenized_eval_data = build_dataset(dataset_config,tokenizer,\"validate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f58ff99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'images'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad66f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af047604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49079e9",
   "metadata": {},
   "source": [
    "## Initialize PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa6ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=tokenized_train_data, data_collator=data_collator)\n",
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=tokenized_train_data, data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf1ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def collator(data):\n",
    "#    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "#ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=tokenized_train_data, data_collator=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05b446",
   "metadata": {},
   "source": [
    "## Load Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e44f84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reward import get_reward_model\n",
    "reward_model = get_reward_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7da3b",
   "metadata": {},
   "source": [
    "## Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae146ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      9\u001b[0m query_tensors \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m images \u001b[38;5;241m=\u001b[39m [transform(Image\u001b[38;5;241m.\u001b[39mopen(img)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m384\u001b[39m, \u001b[38;5;241m384\u001b[39m))) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m image_paths]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#### Get response from gpt2\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'images'"
     ]
    }
   ],
   "source": [
    "    \n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    print(batch.keys())\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    image_tensors = batch[\"query\"]\n",
    "    \n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        response = ppo_trainer.generate(query)\n",
    "        response_tensors.append(response.squeeze())\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "    #batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    outputs = [reward_model.predict_itm(image,text) for image,text in zip(image_tensors,batch[\"response\"])]\n",
    "    rewards = [torch.tensor(output) for output in outputs]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35cd837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
